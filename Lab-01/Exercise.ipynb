{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd487d21",
   "metadata": {},
   "source": [
    "# Lab 01 - Data exploration and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c858f34",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5866566",
   "metadata": {},
   "source": [
    "### Objective of the lab\n",
    "\n",
    "The quality of the data and the amount of useful information that it contains are key factors that determine how well a machine learning algorithm can learn.\n",
    "\n",
    "Therefore, it is absolutely critical that we make sure to examine and preprocess a dataset before we feed it to a learning algorithm. \n",
    "\n",
    "In this laboratory we will explore the data exploration and preprocessing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938e958",
   "metadata": {},
   "source": [
    "#### What is a Dataset?\n",
    "\n",
    "A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every **column** of a table represents a particular **variable**, and each **row** corresponds to a given **record of the data** set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set. Data sets can also consist of a collection of documents or files\n",
    "\n",
    "Several characteristics define a data set's structure and properties. These include the number and types of the attributes or variables, and various statistical measures applicable to them.\n",
    "\n",
    "The values may be **numbers**, such as real numbers or integers, for example representing a person's height in centimeters, but may also be **nominal** data (i.e., not consisting of numerical values), for example representing a person's ethnicity. More generally, values may be of any of the kinds described as a level of measurement.\n",
    "\n",
    "**Missing values** may exist, which must be indicated somehow.\n",
    "\n",
    "[Data set - Wikipedia](https://en.wikipedia.org/wiki/Data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e36221",
   "metadata": {},
   "source": [
    "### Dataset description\n",
    "\n",
    "The sinking of the Titanic is one of the most infamous shipwrecks in history.\n",
    "\n",
    "On April 15, 1912, during her maiden voyage, the widely considered “unsinkable” RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren’t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n",
    "\n",
    "While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7071aecd",
   "metadata": {},
   "source": [
    "| Column Name | Data Type | Description | Possible Values |\n",
    "|-------------|-----------|-------------|----------------|\n",
    "| PassengerId | Integer   | Unique identifier for each passenger | 1, 2, 3, etc. |\n",
    "| Survived    | Integer   | Survival indicator | 0 = No (Did not survive), 1 = Yes (Survived) |\n",
    "| Pclass      | Integer   | Passenger ticket class | 1 = 1st/Upper, 2 = 2nd/Middle, 3 = 3rd/Lower |\n",
    "| Name        | String    | Full name of the passenger | \"Braund, Mr. Owen Harris\", etc. |\n",
    "| Sex         | String    | Gender of the passenger | \"male\", \"female\" |\n",
    "| Age         | Float     | Age of the passenger in years | 22.0, 40.0, etc. (may contain missing values) |\n",
    "| SibSp       | Integer   | Number of siblings/spouses aboard the Titanic | 0, 1, 2, etc. |\n",
    "| Parch       | Integer   | Number of parents/children aboard the Titanic | 0, 1, 2, etc. |\n",
    "| Ticket      | String    | Ticket number | \"A/5 21171\", \"PC 17599\", etc. |\n",
    "| Fare        | Float     | Price paid for the ticket | 7.25, 71.2833, etc. |\n",
    "| Cabin       | String    | Cabin number | \"C85\", \"E46\", etc. (many missing values) |\n",
    "| Embarked    | String    | Port of embarkation | \"C\" = Cherbourg, \"Q\" = Queenstown, \"S\" = Southampton |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad7a3aa",
   "metadata": {},
   "source": [
    "#### What are the different types of data?\n",
    "\n",
    "The two main types of data are:\n",
    "\n",
    "- Qualitative Data\n",
    "- Quantitative Data\n",
    "\n",
    "![types-of-data-img](img/types_of_data.png)\n",
    "\n",
    "---\n",
    "\n",
    "**Qualitative or Categorical Data**\n",
    "   \n",
    "Qualitative or Categorical Data is a type of data that can’t be measured or counted in the form of numbers. These types of data are sorted by category, not by number. That’s why it is also known as Categorical Data. \n",
    "\n",
    "These data consist of *audio, images, symbols, or text*. The gender of a person, i.e., male, female, or others, is qualitative data.\n",
    "\n",
    "Qualitative data tells about the perception of people. This data helps market researchers understand the customers’ tastes and then design their ideas and strategies accordingly. \n",
    "\n",
    "The Qualitative data are further classified into two parts :\n",
    "   \n",
    "- **Nominal Data**\n",
    "\n",
    "    Nominal Data is used to label variables without any order or quantitative value. The color of hair can be considered nominal data, as one color can’t be compared with another color.\n",
    "\n",
    "    With the help of nominal data, we can’t do any numerical tasks or can’t give any order to sort the data. These data don’t have any meaningful order; their values are distributed into distinct categories.\n",
    "\n",
    "- **Ordinal Data**\n",
    "\n",
    "    Ordinal data have natural ordering where a number is present in some kind of order by their position on the scale. These data are used for observation like customer satisfaction, happiness, etc., but we can’t do any arithmetical tasks on them. \n",
    "\n",
    "    Ordinal data is qualitative data for which their values have some kind of relative position. These kinds of data can be considered “in-between” qualitative and quantitative data.\n",
    "\n",
    "    The ordinal data only shows the sequences and cannot use for statistical analysis. Compared to nominal data, ordinal data have some kind of order that is not present in nominal data. \n",
    "\n",
    "--- \n",
    "\n",
    "**Quantitative Data**\n",
    "   \n",
    "Quantitative data is a type of data that can be expressed in numerical values, making it countable and including statistical data analysis. These kinds of data are also known as Numerical data.\n",
    "\n",
    "It answers the questions like “how much,” “how many,” and “how often.” For example, the price of a *phone, the computer’s ram, the height or weight of a person, etc.,* falls under quantitative data. \n",
    "\n",
    "Quantitative data can be used for statistical manipulation. These data can be represented on a wide variety of graphs and charts, such as *bar graphs, histograms, scatter plots, boxplots, pie charts, line graphs, etc.*\n",
    "\n",
    "- **Discrete Data**\n",
    "\n",
    "    The term discrete means distinct or separate. The discrete data contain the values that fall under integers or whole numbers. The total number of students in a class is an example of discrete data. These data can’t be broken into decimal or fraction values.\n",
    "\n",
    "    The discrete data are countable and their subdivision is not possible. These data are represented mainly by a bar graph, number line, or frequency table.\n",
    "\n",
    "- **Continuous Data**\n",
    "\n",
    "    Continuous data are in the form of fractional numbers. It can be the version of an android phone, the height of a person, the length of an object, etc. Continuous data represents information that can be divided into smaller levels. The continuous variable can take any value within a range. \n",
    "\n",
    "The key difference between discrete and continuous data is that discrete data contains the integer or whole number. Still, continuous data stores the fractional numbers to record different types of data such as temperature, height, width, time, speed, etc.\n",
    "\n",
    "[Types Of Data - Great Learning](https://www.mygreatlearning.com/blog/types-of-data/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35bcc3",
   "metadata": {},
   "source": [
    "## Dataset Loading\n",
    "\n",
    "**What is Pandas?**\n",
    "\n",
    "Pandas is an open-source software library designed for data manipulation and analysis. It provides data structures like series and DataFrames to easily clean, transform and analyze large datasets and integrates with other Python libraries, such as NumPy and Matplotlib.\n",
    "\n",
    "It offers functions for data transformation, aggregation and visualization, which are important for analysis.\n",
    "\n",
    "Pandas revolves around two primary Data structures: Series (1D) for single columns and DataFrame (2D) for tabular data enabling efficient data manipulation.\n",
    "\n",
    "With pandas, you can perform a wide range of data operations, including\n",
    "\n",
    "- Reading and writing data from various file formats like CSV, Excel and SQL databases.\n",
    "\n",
    "- Cleaning and preparing data by handling missing values and filtering entries.\n",
    "\n",
    "- Merging and joining multiple datasets seamlessly.\n",
    "\n",
    "- ...\n",
    "\n",
    "[Pandas Tutorial - geeksforgeeks](https://www.geeksforgeeks.org/pandas-tutorial/)\n",
    "\n",
    "For this tutorial, the Titanic dataset will be provided in the format of a `.csv` file. We'll load it using <code>[pandas.read_csv()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d1bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = os.path.join(\"data\", \"lab-01\", \"Titanic-Dataset.csv\")\n",
    "\n",
    "# Load the dataset from the subfolder 'data/lab-01'\n",
    "df = pd.read_csv(filepath_or_buffer=DATASET_PATH)\n",
    "\n",
    "# Display the first 5 rows to check it's loaded correctly\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b7cb1f",
   "metadata": {},
   "source": [
    "## Dataset Overview\n",
    "\n",
    "Now that we have successfully loaded our dataset, the next step is to dive into Exploratory Data Analysis (EDA). EDA is an essential early phase in any data science or machine learning project for several reasons:\n",
    "\n",
    "- **Data Understanding**: EDA helps you learn what each column represents, the types of values present, and how variables might relate to the outcome you're interested in.\n",
    "\n",
    "- **Anomaly Detection**: It allows you to spot outliers, missing values, and data entry errors that could compromise your analysis or model performance.\n",
    "\n",
    "- **Data Cleaning Planning**: Insights from EDA guide how to handle missing data, transform variables, or drop irrelevant columns.\n",
    "\n",
    "- **Feature Selection and Engineering**: By understanding relationships between variables, EDA helps you decide which features are most relevant for modeling.\n",
    "\n",
    "- **Bias and Data Quality Assessment**: EDA can reveal biases, data leakage, or inconsistencies, ensuring a more robust and reliable analysis.\n",
    "\n",
    "- **Hypothesis Generation**: It helps generate initial hypotheses and questions to be tested with more formal statistical methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537ee08f",
   "metadata": {},
   "source": [
    "**Check the Shape of the Dataset**\n",
    "\n",
    "This step gives us a basic understanding of the dataset's size by returning the number of rows (observations) and columns (features). It's an essential first step in EDA to gauge how large and complex your dataset is, and to quickly catch any red flags (e.g., very small datasets or datasets with an unexpectedly large number of columns).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a24cc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of the dataset (rows, columns)\n",
    "# DataFrame.shape: Return a tuple representing the dimensionality of the DataFrame.\n",
    "\n",
    "n_rows, n_cols = df.shape \n",
    "print(f\"Our dataset is composed of {n_rows} rows, and {n_cols} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d74ce8",
   "metadata": {},
   "source": [
    "**General Information About the Dataset**\n",
    "\n",
    "This is a concise summary of the dataset, showing:\n",
    "\n",
    "- Column names and types\n",
    "\n",
    "- Number of non-null entries per column\n",
    "\n",
    "- Memory usage\n",
    "\n",
    "It helps quickly assess missing data, data types, and memory footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcee358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about each column (data type, non-null values, etc.)\n",
    "# DataFrame.info: Print a concise summary of a DataFrame.\n",
    "# This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage. \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1b5f6f",
   "metadata": {},
   "source": [
    "Explore the unique values of our *target* using the <code>[pandas.unique](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.unique.html)</code> module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e1337",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target unique values\")\n",
    "df[\"Survived\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795427f2",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: For the columns *Survived, Pclass, Sex, Embarked*, return the list of unique values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c039567b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d54d8a85",
   "metadata": {},
   "source": [
    "## Data Quality Check\n",
    "\n",
    "Before proceeding with any analysis or modeling, it is essential to verify the quality and integrity of our dataset. Data quality issues can arise in any dataset, including widely used ones like the Titanic dataset. Common problems include:\n",
    "\n",
    "- **Missing values**: Empty cells or NaN entries that may require imputation or removal.\n",
    "\n",
    "- **Inconsistent formats**: Data types or formats that do not align with expectations (e.g., text in a numerical column).\n",
    "\n",
    "- **Duplicate records**: Repeated rows that can distort statistical analysis and model performance.\n",
    "\n",
    "- **Invalid or implausible values**: Outliers or entries that fall outside realistic ranges (such as negative ages).\n",
    "\n",
    "Identifying and addressing these issues early is crucial for ensuring reliable results and robust models. In this section, we will systematically check for missing data, duplicates, and obvious inconsistencies, setting the stage for effective data cleaning and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337b97b",
   "metadata": {},
   "source": [
    "**Check for missing values**\n",
    "\n",
    "This cell checks for missing values in each column of the dataset by using <code>[DataFrame.isnull()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html)</code>. This method returns a boolean DataFrame where:\n",
    "\n",
    "- `True` indicates a missing value (e.g., NaN, None)\n",
    "- `False` indicates a valid entry\n",
    "\n",
    "By summing the boolean values with `.sum()`, we get the total count of missing entries per column.\n",
    "\n",
    "Identifying where and how much data is missing is a crucial step to decide whether to impute, remove, or otherwise handle incomplete data before further analysis or model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values per column\n",
    "print(\"Missing values per column:\\n\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b3e1d",
   "metadata": {},
   "source": [
    "**Check for duplicated rows**\n",
    "\n",
    "This cell checks for duplicate rows in the dataset using <code>[DataFrame.duplicated()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html)</code>, which returns a boolean Series where True indicates a duplicate row. \n",
    "\n",
    "By applying `.sum()`, it counts the total number of duplicate rows.\n",
    "\n",
    "Identifying and removing duplicates is important because repeated entries can skew statistical analyses and affect model performance. This step helps ensure that each observation is unique and reliable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6b44b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicated rows\n",
    "print(\"Number of duplicated rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6234a7",
   "metadata": {},
   "source": [
    "At first glance, it appears that there are no duplicates within the dataset, but let us take a further look at the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1857055",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7015c1",
   "metadata": {},
   "source": [
    "*PassengerId* appears to be a numerical identifier of the passenger, but consists only of an incremental number. This data is useless for our classification, so we can remove it. \n",
    "\n",
    "By calling the <code>[DataFrame.is_unique](https://pandas.pydata.org/docs/reference/api/pandas.Series.is_unique.html)</code> property, we can check if the *PassengerId* column is unique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093d37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if PassengerId is just a sequence of numbers\n",
    "df['Survived'].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a985843",
   "metadata": {},
   "source": [
    "Obviously, if at least one column of our dataset is not unique, then there is no point in using the <code>[DataFrame.duplicated()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html)</code> method to check for duplicates.\n",
    "\n",
    "We can drop the column using the <code>[DataFrame.drop()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html)</code> method, since we assume that the *PassengerId* column will not useful for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069a8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"PassengerId\"], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e594b7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check again for duplicated rows\n",
    "print(\"Number of duplicated rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15f3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated rows\n",
    "df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "\n",
    "print(\"Number of duplicated rows after removing duplicates:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac369c16",
   "metadata": {},
   "source": [
    "**Examining rows with inconsistencies or invalid entries**\n",
    "\n",
    "This part demonstrates how to identify invalid or inconsistent data entries in the Titanic dataset:\n",
    "\n",
    "- **Invalid 'Fare' values**: The first part filters rows where the 'Fare' column contains values that are less than or equal to zero. Negative fares are invalid, as they don't make sense in the context of ticket prices.\n",
    "\n",
    "- **Invalid 'Embarked' values**: The second part checks the 'Embarked' column for values that are not among the valid embarkation ports ('C', 'Q', or 'S'). Any entry outside these three values would be inconsistent and require attention.\n",
    "\n",
    "- **Missing 'Age' values**: The third part filters rows where the 'Age' column contains missing values (NaN). These missing values need to be handled, either through imputation or removal, to ensure that the dataset is complete for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6a6a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check for invalid fares\n",
    "print(\"Rows with invalid 'Fare' values (negative):\")\n",
    "display(df[df['Fare'] < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbb181",
   "metadata": {},
   "source": [
    "At this point, we can decide either to drop the rows with invalid entries or to replace them with a default value (e.g., 0). \n",
    "\n",
    "In this case, we will just use the average price of the tickets for the *Fare* column.\n",
    "\n",
    "We will use the <code>[DataFrame.mean()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html)</code> method to compute the average price of the tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580c4be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the average fare\n",
    "\n",
    "average_fare = df['Fare'].mean().round(2)\n",
    "print(f\"Average fare: {average_fare}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fb54bb",
   "metadata": {},
   "source": [
    "To access specific rows and columns in a dataset, we can use two powerful tools provided by pandas: <code>[.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)</code> and <code>[.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)</code>.\n",
    "\n",
    "The <code>[.loc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)</code> accessor allows us to select data using labels (e.g., row indices or column names). It is especially useful when working with datasets that have meaningful index values or when we want to reference columns by name.\n",
    "\n",
    "The <code>[.iloc](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iloc.html)</code> accessor, on the other hand, is purely integer-based. It allows us to select data by position, just like regular Python lists."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a34e971",
   "metadata": {},
   "source": [
    "We are going to use a **mask** to filter the rows of our dataset. A mask is a boolean array that indicates which rows meet a certain condition. In this case, we will create a mask to identify rows where the 'Fare' column has invalid values (less than zero) and then use that mask to filter the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549a6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df['Fare'] < 0\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c833359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute the negative fare with the average fare\n",
    "df.loc[mask, 'Fare'] = average_fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205c152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Check for invalid embarkation ports (should be 'C', 'Q', or 'S')\n",
    "print(\"Rows with invalid 'Embarked' values:\")\n",
    "\n",
    "mask = ~df['Embarked'].isin(['C', 'Q', 'S'])\n",
    "display(df[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f047de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, we can drop the rows with invalid embarkation ports\n",
    "\n",
    "df.drop(df[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86fdb7",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "\n",
    "During performing data quality checks, it is crucial to explore the distribution and composition of our dataset, both numerically and categorically.\n",
    "\n",
    "This step is crucial because:\n",
    "\n",
    "- It reveals the central tendency (mean, median) and spread (variance, range) of numerical features.\n",
    "\n",
    "- It helps identify outliers, skewed distributions, and potential data imbalance.\n",
    "\n",
    "- It offers insights into categorical distributions, which are vital for understanding group sizes and planning encoding strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377eae28",
   "metadata": {},
   "source": [
    "**Generate Summary Statistics**\n",
    "\n",
    "The <code>[DataFrame.describe()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)</code> method provides a quick overview of the dataset's numerical features.\n",
    "\n",
    "This command gives you summary statistics (mean, std, min, max, percentiles) for all columns, including categorical ones `(include=\"all\")`. Transposing the result `(.T)` makes it easier to read.\n",
    "\n",
    "It's good practice for:\n",
    "\n",
    "- Understanding distributions and value ranges\n",
    "\n",
    "- Spotting outliers or suspicious values\n",
    "\n",
    "- Gauging variability in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8305e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical columns\n",
    "# DataFrame.describe: Generate descriptive statistics.\n",
    "# Descriptive statistics include those that summarize the central tendency, \n",
    "# dispersion and shape of a dataset’s distribution, excluding NaN values.\n",
    "# Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. \n",
    "\n",
    "df.describe(include=\"all\").T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418d97e6",
   "metadata": {},
   "source": [
    "This step examines how frequently each category appears in selected columns using <code>[.value_counts()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.value_counts.html)</code>:\n",
    "\n",
    "It reveals class imbalances, missing values (especially with `dropna=False`), and dominant categories — all of which can guide encoding strategies or model interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60966a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for categorical features\n",
    "print(\"\\nValue counts for 'Sex':\")\n",
    "print(df['Sex'].value_counts())\n",
    "\n",
    "print(\"\\nValue counts for 'Pclass':\")\n",
    "print(df['Pclass'].value_counts())\n",
    "\n",
    "print(\"\\nValue counts for 'Embarked':\")\n",
    "print(df['Embarked'].value_counts(dropna=False))  # include NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1de943",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: Together with the default percentile, return in the output also the 1st and 99th percentile\n",
    "\n",
    "Check at this link the documentation of the module <code>[DataFrame.describe](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.describe.html)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb40dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da3dba6c",
   "metadata": {},
   "source": [
    "## Basic Data Visualization\n",
    "\n",
    "After summarizing the dataset using statistics, the next step is to visualize the data. Visualization helps us spot patterns, trends, and anomalies that are hard to detect in raw numbers alone.\n",
    "\n",
    "These visual tools make data intuitive and accessible, even for those without strong statistical backgrounds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7454b04e",
   "metadata": {},
   "source": [
    "**Target Variable Distribution: `Survived`**\n",
    "\n",
    "Before analyzing other features, it's important to explore the distribution of our **target variable**, `Survived`. This bar chart illustrates the number of passengers who survived (1) versus those who did not survive (0).\n",
    "\n",
    "Why this matters:\n",
    "- Class imbalance occurs when one class significantly outnumbers the other(s). For example, if most passengers did not survive, models might learn to always predict the majority class just to achieve higher accuracy — without actually learning useful patterns.\n",
    "\n",
    "- This can lead to misleading performance metrics, especially if we rely solely on accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fbe07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of each survival class\n",
    "index = df[\"Survived\"].value_counts().index\n",
    "values = df[\"Survived\"].value_counts().values\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(index, values)\n",
    "\n",
    "plt.xlabel(\"Survival (0 = No, 1 = Yes)\")\n",
    "plt.ylabel(\"Number of Passengers\")\n",
    "plt.title(\"Passenger Survival Counts\")\n",
    "plt.xticks(index, labels=[\"Did Not Survive\", \"Survived\"])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89e5f3",
   "metadata": {},
   "source": [
    "**Numerical Variables Distribution: `Age`**\n",
    "\n",
    "To understand the spread and shape of the data, we visualize the distribution of the Age feature using a histogram combined with a Kernel Density Estimate (KDE):\n",
    "\n",
    "- The histogram shows how passenger ages are distributed across intervals. It highlights concentrations of values — for example, many passengers appear to be between 20 and 40 years old.\n",
    "\n",
    "- The KDE curve (in red) offers a smooth approximation of the underlying distribution. Unlike the rigid bins of a histogram, KDE helps us see the overall shape and continuity of the data — such as skewness or multiple peaks.\n",
    "\n",
    "This kind of plot is valuable for:\n",
    "\n",
    "- Identifying common age ranges\n",
    "\n",
    "- Detecting skewed distributions or multi-modal trends\n",
    "\n",
    "- Spotting areas where data is sparse, which may influence model performance\n",
    "\n",
    "- Identifying potential outliers or unusual age distributions\n",
    "\n",
    "By combining histogram and KDE, we get both discrete and continuous perspectives on the same feature, making it easier to decide on transformations (e.g., normalization, binning) or whether age should be treated differently in model pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee827da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_data = df['Age'].dropna()\n",
    "\n",
    "kde = gaussian_kde(age_data)\n",
    "age_range = np.linspace(age_data.min(), age_data.max(), 100)\n",
    "kde_values = kde(age_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bins = np.arange(age_data.min(), age_data.max() + 1)\n",
    "ax.hist(age_data, bins=bins, edgecolor='black', density=True, alpha=0.6, label='Histogram')\n",
    "ax.plot(age_range, kde_values, color='red', label='KDE')\n",
    "\n",
    "ax.set_title('Distribution of Age with KDE')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa61d441",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: Remove the outliers from the *Age* column and plot again the histogram with the KDE curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5b61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb31483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows \n",
    "df.drop(df[mask].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f3705",
   "metadata": {},
   "source": [
    "**Categorical Features Distribution**\n",
    "\n",
    "Visualizing categorical variables helps us understand the composition and potential impact of each category on the target variable. By plotting count distributions, we can quickly detect:\n",
    "\n",
    "- Imbalances between categories (e.g., more males than females)\n",
    "- Dominant groups within features like Sex, Pclass, Embarked, etc.\n",
    "- Potential patterns or groupings relevant for modeling and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ca462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_counts = df['Sex'].value_counts()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(sex_counts.index, sex_counts.values)\n",
    "\n",
    "ax.set_title('Passenger Count by Sex')\n",
    "ax.set_xlabel('Sex')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False)  \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f2d4c",
   "metadata": {},
   "source": [
    "**Boxplot of Age by Survival**\n",
    "\n",
    "In descriptive statistics, a box plot or boxplot is a method for demonstrating graphically the locality, spread and skewness groups of numerical data through their quartiles.\n",
    "In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles.\n",
    "Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot. Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution.\n",
    "\n",
    "[Boxplot - Wikipedia](https://en.wikipedia.org/wiki/Box_plot)\n",
    "\n",
    "**Elements of Box Plot**\n",
    "\n",
    "A box plot gives a five-number summary of a set of data which is:\n",
    "\n",
    "- Minimum – It is the minimum value in the dataset excluding the outliers.\n",
    "- First Quartile (Q1) – 25% of the data lies below the First (lower) Quartile.\n",
    "- Median (Q2) – It is the mid-point of the dataset. Half of the values lie below it and half above.\n",
    "- Third Quartile (Q3) – 75% of the data lies below the Third (Upper) Quartile.\n",
    "- Maximum – It is the maximum value in the dataset excluding the outliers.\n",
    "\n",
    "![types-of-data-img](img/boxplot_example.jpg)\n",
    "\n",
    "\n",
    "The area inside the box (50% of the data) is known as the Inter Quartile Range. The IQR is calculated as:\n",
    "\n",
    "$IQR = Q3-Q1$\n",
    "\n",
    "Outlies are the data points below and above the lower and upper limit. The lower and upper limit is calculated as:\n",
    "\n",
    "$Lower Limit = Q1 - 1.5*IQR$\n",
    "\n",
    "$Upper Limit = Q3 + 1.5*IQR$\n",
    "\n",
    "The values below and above these limits are considered outliers and the minimum and maximum values are calculated from the points which lie under the lower and upper limit.\n",
    "\n",
    "[Boxplot -geeksforgeeks](https://www.geeksforgeeks.org/box-plot/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78ee627",
   "metadata": {},
   "source": [
    "Here, we use a boxplot to examine how age varies between passengers who survived and those who did not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5daf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_0 = df[df['Survived'] == 0]['Age'].dropna()\n",
    "survived_1 = df[df['Survived'] == 1]['Age'].dropna()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.boxplot([survived_0, survived_1], labels=['Not Survived', 'Survived'])\n",
    "\n",
    "ax.set_title('Age Distribution by Survival')\n",
    "ax.set_ylabel('Age')\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7b8d4b",
   "metadata": {},
   "source": [
    "**Correlation Heatmap**\n",
    "\n",
    "A correlation heatmap is a graphical tool used in statistics and data analysis to visualize the **strength** and **direction** of relationships between multiple numerical variables simultaneously. It displays a correlation matrix where each variable is represented both as a row and a column, and each cell shows the correlation coefficient between the pair of variables it intersects.\n",
    "\n",
    "![types-of-data-img](img/correlation.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b9044",
   "metadata": {},
   "source": [
    "The values in the heatmap cells are correlation coefficients, typically Pearson’s correlation, which range from -1 to 1. A value of 1 indicates a perfect positive **linear relationship** (as one variable increases, the other increases), -1 indicates a perfect negative **linear relationship** (one variable increases as the other decreases), and 0 indicates no linear relationship\n",
    "\n",
    "In preprocessing, identifying highly correlated variables is crucial. Variables with very high positive or negative correlations (e.g., above 0.7 or below -0.7) may indicate redundancy or multicollinearity, which can affect model performance. Heatmaps help pinpoint such variables for potential removal or transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910a7faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.select_dtypes(include=[np.number]).corr()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "cax = ax.matshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "\n",
    "fig.colorbar(cax)\n",
    "\n",
    "ax.set_xticks(range(len(corr.columns)))\n",
    "ax.set_yticks(range(len(corr.columns)))\n",
    "\n",
    "ax.set_xticklabels(corr.columns, rotation=45, ha='left')\n",
    "ax.set_yticklabels(corr.columns)\n",
    "\n",
    "ax.set_title('Correlation Heatmap of Numeric Features', pad=20)\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3532d9c4",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: See the actual correlation between the *Pclass* feature and the *Survived* target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33251565",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0224a37f",
   "metadata": {},
   "source": [
    "## Exploring Feature Relationships\n",
    "\n",
    "Before performing any advanced analysis or modeling, it is essential to explore and understand the underlying structure of the dataset. One fundamental tool for this process is the <code>[DataFrame.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)</code> function in the pandas library, which allows for the aggregation and summarization of data based on one or more categorical features. By grouping data, we can uncover patterns, compare subsets, and derive meaningful insights that would otherwise remain hidden in raw tables. The following exercises are designed to develop familiarity with <code>[DataFrame.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)</code> operations and demonstrate how systematic data exploration leads to a deeper understanding of trends, distributions, and relationships within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68fa63",
   "metadata": {},
   "source": [
    "### Grouped statistics\n",
    "\n",
    "Pandas <code>[DataFrame.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)</code> function is a powerful tool used to split a DataFrame into groups based on one or more columns, allowing for efficient data analysis and aggregation. It follows a “split-apply-combine” strategy, where data is divided into groups, a function is applied to each group, and the results are combined into a new DataFrame.\n",
    "\n",
    "The groupby() function in Pandas involves three main steps: Splitting, Applying, and Combining.\n",
    "\n",
    "- Splitting: This step involves dividing the DataFrame into groups based on some criteria. The groups are defined by unique values in one or more columns.\n",
    "\n",
    "- Applying: In this step, a function is applied to each group independently. You can apply various functions to each group, such as:\n",
    "    - Aggregation: Calculate summary statistics (e.g., sum, mean, count) for each group.\n",
    "    - Transformation: Modify the values within each group.\n",
    "    - Filtering: Keep or discard groups based on certain conditions.\n",
    "\n",
    "- Combining: Finally, the results of the applied function are combined into a new DataFrame or Series.\n",
    "\n",
    "[Pandas DataFrame.groupby() Method - geeksforgeeks](https://www.geeksforgeeks.org/python-pandas-dataframe-groupby/)\n",
    "\n",
    "The abstract definition of grouping is to provide a mapping of labels to group names.\n",
    "For **DataFrame objects**, a string indicating either a column name or an index level name to be used to group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e15534",
   "metadata": {},
   "source": [
    "#### Grouping by single columns\n",
    "In this example, we will demonstrate how to group data by a single column using the <code>[DataFrame.groupby()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html)</code> method. We will group by our categorical features, and explore how their relationship with the target feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89539e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average survival rate for each sex (female/male)\n",
    "df.groupby(by=\"Sex\")[\"Survived\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8986a1",
   "metadata": {},
   "source": [
    "This tells us that \n",
    "- 74% of women passengers survived\n",
    "- 18% of men passengers survived\n",
    "  \n",
    "thus we could infer that women were more likely to survive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be7c2d",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: Calculate the average survival rate for each passenger ticket class (1st, 2nd, 3rd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d982743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1838a121",
   "metadata": {},
   "source": [
    "![Titanic cabin assignments through class](img/titanic-figure-one-side-view.png)\n",
    "\n",
    "[Titanic cabin placement](https://www.encyclopedia-titanica.org/class-gender-titanic-disaster-1912~chapter-2~part-2.html)\n",
    "\n",
    "​The Titanic disaster highlighted the influence of social class on survival rates. Several factors contributed to this disparity. The ship's design placed first-class accommodations closer to the lifeboats, facilitating quicker access during the evacuation. Additionally, third-class passengers faced physical barriers and lacked clear guidance, which hindered their ability to reach lifeboats in time. These structural and procedural disadvantages underscore how deeply social hierarchies impacted survival outcomes during the tragedy.​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc76e6d",
   "metadata": {},
   "source": [
    "As demonstrated, it is essential to explore data in depth alongside its context, as this approach can uncover meaningful insights into model performance during inference. Understanding the background not only helps interpret results more accurately, but also sheds light on potential biases and patterns that might otherwise go unnoticed. This is particularly crucial when working with historical or real-world datasets, where external factors can significantly influence outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3600768d",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "It is not uncommon in real-world applications for our samples to be missing one or more values for various reasons. There could have been an error in the data collection process, certain measurements are not applicable, or particular fields could have been simply left blank in a survey, for example. We typically see missing values as the blank spaces in our data table or as placeholder strings such as NaN, which stands for not a number, or NULL (a commonly used indicator of unknown values in relational databases).\n",
    "\n",
    "Unfortunately, most computational tools are unable to handle such missing values, or produce unpredictable results if we simply ignore them. Therefore, it is crucial that we take care of those missing values before we proceed with further analyses. In this section, we will work through several practical techniques for dealing with missing values by removing entries from our dataset or imputing missing values from other samples and features.\n",
    "\n",
    "(the material of this section is a reference to *Raschka, S., & Mirjalili, V. (2019). Python machine learning: Machine learning and deep learning with Python, scikit-learn, and TensorFlow 2 (3rd ed.). Packt Publishing.*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ec1dba",
   "metadata": {},
   "source": [
    "### Eliminating samples or features with missing values\n",
    "\n",
    "One of the easiest ways to deal with missing data is to simply remove the corresponding features (columns) or samples (rows) from the dataset entirely; rows with missing values can be easily dropped via the dropna method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa295ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data set shape before dropping rows with missing values:\", df.shape)\n",
    "temp = df.dropna(axis=0)\n",
    "print(\"Data shape after dropping rows with missing values:\", temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72188fc4",
   "metadata": {},
   "source": [
    "Similarly, we can drop columns that have at least one <code>NaN</code> in any row by setting the <code>axis</code> argument to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5915326",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data set shape before dropping columns with missing values:\", df.shape)\n",
    "temp = df.dropna(axis=1)\n",
    "print(\"Data shape after dropping columns with missing values:\", temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f587b126",
   "metadata": {},
   "source": [
    "The dropna method supports several additional parameters that can come in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610d62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only drop rows where all columns are NaN\n",
    "df.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a6f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only drop rows where NaN appear in specific columns (here: 'Cabin')\n",
    "df.dropna(subset=['Cabin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1d5cb5",
   "metadata": {},
   "source": [
    "Although the removal of missing data seems to be a convenient approach, it also comes with certain disadvantages; for example, we may end up removing too many samples, which will make a reliable analysis impossible. Or, if we remove too many feature columns, we will run the risk of losing valuable information that our classifier needs to discriminate between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6497d",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: Impute the missing values in the <code>Cabin</code> column filling the missing values with the string \"Unknown\". \n",
    "You can use the <code>[DataFrame.fillna()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html)</code> method to fill the missing values with a specific value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643f8c27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9489ec02",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that are suitable for machine learning models. In other words, it is the process of selecting, extracting, and transforming the most relevant features from the available data to build more accurate and efficient machine learning models.\n",
    "\n",
    "The success of machine learning models heavily depends on the quality of the features used to train them. Feature engineering involves a set of techniques that enable us to create new features by combining or transforming the existing ones. These techniques help to highlight the most important patterns and relationships in the data, which in turn helps the machine learning model to learn from the data more effectively.\n",
    "\n",
    "[What is Feature Engineering? - geeksforgeeks](https://www.geeksforgeeks.org/what-is-feature-engineering/)\n",
    "\n",
    "| Column Name              | Data Type                   | Description                                                             | Possible Values / Examples                             |\n",
    "|--------------------------|-----------------------------|-------------------------------------------------------------------------|--------------------------------------------------------|\n",
    "| Feature Extraction       | String → Categorical         | Extract useful parts from complex fields or derive new features (e.g., dimensionality reduction) | From `Name` → `Title` = \"Mr\", \"Mrs\", \"Miss\", etc.; PCA for dimensionality reduction |\n",
    "| Feature Transformation   | Numeric                     | Apply mathematical or rule-based transformation to values              | `Fare` → log(Fare), `Age` → square root(Age)           |\n",
    "| Encoding Categorical     | Categorical → Numeric       | Convert string labels into numeric format for modeling                 | `Sex` = \"male\", \"female\" → 1, 0 using `get_dummies()`  |\n",
    "| Binning                  | Numeric → Categorical       | Divide continuous values into discrete intervals                       | `Age` → \"Child\", \"Adult\", \"Senior\"                     |\n",
    "| Feature Interaction      | Multiple Columns            | Combine two or more columns to create meaningful relationships         | `FamilySize` = `SibSp` + `Parch` + 1                   |\n",
    "| Datetime Decomposition   | Datetime → Numeric/Categorical | Break down datetime into parts like year, month, day, etc.            | `2023-04-19` → year = 2023, month = 4                  |\n",
    "| Scaling / Normalization  | Numeric                     | Scale numeric values to a common range or distribution                 | `Fare` → 0–1 using MinMaxScaler                        |\n",
    "| Missing Value Imputation | Any                        | Fill missing values with mean, median, mode, or prediction             | `Age` → fill missing with median age (e.g., 28.0)      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c465626d",
   "metadata": {},
   "source": [
    "### Handling categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc752004",
   "metadata": {},
   "source": [
    "**Feature Extraction & Encoding for Nominal Variables:**\n",
    "\n",
    "Feature extraction for nominal (categorical) data involves transforming raw categorical variables into new, meaningful features that can be effectively used in machine learning models. This process often includes extracting patterns or subcomponents from the nominal data to capture relevant information such as social status, gender, or groupings that may influence the prediction target.\n",
    "\n",
    "Nominal data in our dataset, such as **names** or **cabin identifiers**, are non-numeric and cannot be directly used by most machine learning algorithms. Feature extraction techniques for nominal data typically involve:\n",
    "\n",
    "- Parsing and extracting meaningful components: For example, extracting titles (Mr, Mrs, Miss) from passenger names to capture social status or demographics.\n",
    "\n",
    "- Decomposing complex categorical variables: Breaking down a cabin number into a deck letter and a numeric part to capture location information.\n",
    "\n",
    "- Encoding categorical variables: Converting extracted features into formats suitable for modeling, such as one-hot encoding or numeric encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f5a24d",
   "metadata": {},
   "source": [
    "This code extracts passenger titles (such as Mr., Mrs., Miss, etc.) from the Name column in the dataset. It uses the <code>[str.split()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.split.html)</code> method to capture the string that appears between a comma and a period, which typically corresponds to the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e620008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract title from passenger name\n",
    "name = df['Name']\n",
    "title = name.str.split(',', expand=True)[1].str.split('.', expand=True)[0]\n",
    "\n",
    "# View unique titles\n",
    "print(\"Unique titles found:\")\n",
    "title.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3da16dc",
   "metadata": {},
   "source": [
    "<b style=\"color:red;\">Exercise</b>: To help identify the cabin location of the passengers, we can extract the deck letter and the cabin number from the *Cabin* column. The cabin number is a string that contains both letters and numbers (e.g., 'C85'). We can use regular expressions to extract both the deck letter and the cabin number. \n",
    "Take a look at the <code>[str.extract()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.str.extract.html)</code> and <code>[pandas.to_numeric()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_numeric.html)</code> methods to extract the cabin number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e9286",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96447256",
   "metadata": {},
   "source": [
    "Our model can’t handle text labels like 'male' or 'female',  <code>[pandas.get_dummies()](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html)</code> convert categorical variable into dummy/indicator variables.\n",
    "\n",
    "Each variable is converted in as many 0/1 variables as there are different values. Columns in the output are each named after a value; if the input is a DataFrame, the name of the original variable is prepended to the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a227b75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode 'Sex' column into binary variables (0 or 1)\n",
    "df_encoded = pd.get_dummies(df, columns=['Sex']) # by setting drop_first = 1, the first dummy column will be dropped, since it has redundant information\n",
    "print(df_encoded[[\"Sex_female\", \"Sex_male\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd3b1b2",
   "metadata": {},
   "source": [
    "Alternatively we can use <code>[sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html)</code> to perform one-hot encoding. This is a more flexible and powerful approach, especially when dealing with larger datasets or when you need to handle unknown categories. Take a look at the documentation to discover all the potential options!\n",
    "\n",
    "With one-hot, we convert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns. Each integer value is represented as a binary vector. All the values are zero, and the index is marked with a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a9a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(categories=[[\"Q\", \"S\", \"C\"]], handle_unknown=\"ignore\")\n",
    "embarked = ohe.fit_transform(df[[\"Embarked\"]])\n",
    "embarked_df = pd.DataFrame(embarked.toarray(), columns=ohe.get_feature_names_out([\"Embarked\"]))\n",
    "\n",
    "embarked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063101a9",
   "metadata": {},
   "source": [
    "### Handling numerical data (Feature Scaling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc59cd0",
   "metadata": {},
   "source": [
    "**Feature scaling** is an important step in the preprocessing pipeline for many machine learning models, even though some algorithms, like decision trees and random forests, do not require it. These models are \"scale invariant,\" meaning that the performance of the algorithm does not change with different feature scales. However, many other machine learning algorithms benefit from feature scaling because their performance can be affected by the differences in the magnitude of features.\n",
    "\n",
    "To better understand the significance of feature scaling, let's take a look at two concrete examples:\n",
    "\n",
    "1. Gradient Descent (Linear Regression or Logistic Regression)\n",
    "Imagine you are working with a dataset that includes two features:\n",
    "\n",
    "    Feature 1 (Income): Ranges from 10,000 to 100,000 (in dollars).\n",
    "\n",
    "    Feature 2 (Age): Ranges from 18 to 70 (in years).\n",
    "\n",
    "    When you use gradient descent to minimize the cost function (e.g., mean squared error), the algorithm adjusts weights based on the features. If the income feature has values that are much larger than the age feature, the model will tend to focus more on optimizing the weight of the income feature because the numerical range of income is much larger. As a result, the model may not learn as effectively from the age feature.\n",
    "\n",
    "    If both features were scaled to have a similar range (e.g., between 0 and 1 or with a standard deviation of 1), the gradient descent algorithm would update both weights more evenly, leading to a more balanced and effective model.\n",
    "\n",
    "2. k-Nearest Neighbors (KNN)\n",
    "    Consider a dataset with the following two features:\n",
    "\n",
    "    Feature 1 (Distance to school): Ranges from 0 to 5 kilometers.\n",
    "\n",
    "    Feature 2 (Annual salary): Ranges from 20,000 to 200,000 (in dollars).\n",
    "\n",
    "    When using k-nearest neighbors (KNN) with the Euclidean distance metric to classify or predict, the distances between samples are calculated based on the features. However, the distance calculation involves squaring the differences of each feature's value, and the larger the feature's range, the more influence it has on the final distance. In this case, the salary feature will dominate the distance calculation because its range is much larger than that of the distance to school feature.\n",
    "\n",
    "    For example, if two data points differ significantly in salary but have very similar distances to school, the KNN algorithm might incorrectly assign a higher weight to the salary difference due to its larger scale, making the distance metric less representative of the actual similarities between data points.\n",
    "\n",
    "    If both features were scaled to the same range, the distance between samples would be influenced equally by both features, leading to more accurate classifications or predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552b828",
   "metadata": {},
   "source": [
    "Now, there are two common approaches to bring different features onto the same scale: **normalization** and **standardization**. Those terms are often used quite loosely in different fields, and the meaning has to be derived from the context. Most often, **normalization** refers to the rescaling of the features to a range of \\([0, 1]\\), which is a special case of **min-max scaling**.\n",
    "\n",
    "To normalize our data, we can simply apply the min-max scaling to each feature column, where the new value of a sample can be calculated as follows:\n",
    "\n",
    "$X_{\\text{normalized}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}$\n",
    "\n",
    "\n",
    "Where:\n",
    "- $X_{\\text{normalized}}$ is the normalized value of the feature.\n",
    "\n",
    "- $X$ is the original value of the feature.\n",
    "\n",
    "- $X_{\\text{min}}$ is the minimum value of the feature.\n",
    "\n",
    "- $X_{\\text{max}}$ is the maximum value of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ef9a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "age_data = df['Age'].dropna()\n",
    "\n",
    "kde = gaussian_kde(age_data)\n",
    "age_range = np.linspace(age_data.min(), age_data.max(), 100)\n",
    "kde_values = kde(age_range)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "bins = np.arange(age_data.min(), age_data.max() + 1)\n",
    "ax.hist(age_data, bins=bins, edgecolor='black', density=True, alpha=0.6, label='Histogram')\n",
    "ax.plot(age_range, kde_values, color='red', label='KDE')\n",
    "\n",
    "ax.set_title('Distribution of Age with KDE')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False) \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89ae9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's apply MinMaxScaling to our Age feature\n",
    "mm_scaler = MinMaxScaler()\n",
    "age = df[\"Age\"].values.reshape(-1, 1)\n",
    "normalized_age = mm_scaler.fit_transform(age)\n",
    "print(f\"Normalized age maximum: {max(normalized_age)}\")\n",
    "print(f\"Normalized age minimum: {min(normalized_age)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb027a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_age = normalized_age[~np.isnan(normalized_age)]\n",
    "\n",
    "kde = gaussian_kde(normalized_age)\n",
    "age_range = np.linspace(normalized_age.min(), normalized_age.max(), 100)\n",
    "kde_values = kde(age_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.hist(normalized_age, bins=30, edgecolor='black', density=True, alpha=0.6, label='Histogram')\n",
    "ax.plot(age_range, kde_values, color='red', label='KDE')\n",
    "\n",
    "ax.set_title('Distribution of Age with KDE')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c439f90d",
   "metadata": {},
   "source": [
    "Although normalization via min-max scaling is a commonly used technique that is useful when we need values in a bounded interval, **standardization** can be more practical for many machine learning algorithms, especially for optimization algorithms such as gradient descent. The reason is that many linear models initialize the weights to 0 or small random values close to 0. Using standardization, we center the feature columns at a mean of 0 with a standard deviation of 1, so that the feature columns take the form of a normal distribution, which makes it easier to learn the weights.\n",
    "\n",
    "Furthermore, standardization maintains useful information about outliers and makes the algorithm less sensitive to them, in contrast to min-max scaling, which scales the data to a limited range of values.\n",
    "\n",
    "The procedure for **standardization** can be expressed by the following equation:\n",
    "\n",
    "$X_{\\text{standardized}} = \\frac{X - \\mu}{\\sigma}$\n",
    "\n",
    "Where:\n",
    "- $X_{\\text{standardized}}$ is the standardized value of the feature.\n",
    "\n",
    "- $X$ is the original value of the feature.\n",
    "\n",
    "- $\\mu$ is the sample mean of the feature.\n",
    "\n",
    "- $\\sigma$ is the standard deviation of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f0a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_scaler = StandardScaler()\n",
    "age = df[\"Age\"].dropna().values.reshape(-1, 1)\n",
    "standardized_age = ss_scaler.fit_transform(age)\n",
    "\n",
    "print(f\"Standardized age maximum: {max(standardized_age)}\")\n",
    "print(f\"Standardized age minimum: {min(standardized_age)}\")\n",
    "print(f\"Standardized age mean: {round(np.mean(standardized_age), 10)}\")\n",
    "print(f\"Standardized age standard deviation: {round(np.std(standardized_age), 10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40efa7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_age = standardized_age[~np.isnan(standardized_age)]\n",
    "\n",
    "kde = gaussian_kde(standardized_age)\n",
    "age_range = np.linspace(standardized_age.min(), standardized_age.max(), 100)\n",
    "kde_values = kde(age_range)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.hist(standardized_age, bins=30, edgecolor='black', density=True, alpha=0.6, label='Histogram')\n",
    "ax.plot(age_range, kde_values, color='red', label='KDE')\n",
    "\n",
    "ax.set_title('Distribution of Age with KDE')\n",
    "ax.set_xlabel('Age')\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend()\n",
    "ax.set_axisbelow(True)\n",
    "ax.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5df500",
   "metadata": {},
   "source": [
    "### Pipeline, Column Transformers and Custom Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d657c5cd",
   "metadata": {},
   "source": [
    "The process of transforming raw data into a model-ready format often involves a series of steps, including data preprocessing, feature selection, and model training. Managing these steps efficiently and ensuring reproducibility can be challenging. This is where <code>[sklearn.pipeline.Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)</code> from the scikit-learn library comes into play.\n",
    "\n",
    "The **Pipeline** class in scikit-learn is a powerful tool designed to streamline the machine learning workflow. It allows you to chain together multiple steps, such as data transformations and model training, into a single, cohesive process. This not only simplifies the code but also ensures that the same sequence of steps is applied consistently to both training and testing data, thereby reducing the risk of data leakage and improving reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe960fc",
   "metadata": {},
   "source": [
    "To illustrate the use of scikit-learn pipelines, let's walk through a simple example where we apply a series of transformations to the \"Age\" feature in a dataset. In this example, we will:\n",
    "\n",
    "- Impute missing values in the \"Age\" column with the mean value.\n",
    "\n",
    "- Standardize the \"Age\" values, so they have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "We will use the **Pipeline** class from scikit-learn to combine these two steps into one unified process, making it easier to manage and apply consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e933a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before ...\")\n",
    "print(f\"Age min/max: {df['Age'].min():.2f}, {df['Age'].max():.2f}\")\n",
    "print(f\"Age missing values: {df['Age'].isna().sum()}\")\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "    (\"mean_imp\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"ss_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "age_transformed = age_pipeline.fit_transform(df[[\"Age\"]])  # Returns NumPy array\n",
    "\n",
    "print(\"\\nAfter ...\")\n",
    "print(f\"Age min/max: {age_transformed.min():.2f}, {age_transformed.max():.2f}\") \n",
    "print(f\"Age missing values: {np.isnan(age_transformed).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0591a0a",
   "metadata": {},
   "source": [
    "**Custom Transformers in scikit-learn**\n",
    "\n",
    "In many cases, standard transformers like SimpleImputer and StandardScaler are not sufficient for specific tasks. For such situations, we can create custom transformers that can be used just like the built-in ones, while performing more complex transformations.\n",
    "\n",
    "A custom transformer is a class that inherits from BaseEstimator and TransformerMixin in scikit-learn. The primary responsibility of a custom transformer is to define two methods:\n",
    "\n",
    "- <code>fit(...)</code>: This method is used to learn any necessary parameters from the data. For simple transformations that don't require fitting (like string manipulation), you can leave this method empty.\n",
    "  \n",
    "- <code>transform(...)</code>: This method performs the actual transformation of the data.\n",
    "  \n",
    "- <code>get_feature_names_out(...)</code>: (Optional but recommended) This method defines the names of the output features, useful when using pipelines or ColumnTransformers.\n",
    "\n",
    "Let’s look at an example where we create a custom transformer to preprocess the \"Cabin\" feature in the Titanic dataset.\n",
    "\n",
    "In the Titanic dataset, the \"Cabin\" column contains string values, which include both the deck (a letter) and the cabin number (a number). We can create a custom transformer to extract and separate these values into two new columns: CabinDeck and CabinNumber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ee7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CabinTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = pd.Series(X.flatten()) \n",
    "        \n",
    "        deck = X.str[0]  # First letter (deck)\n",
    "        number = X.str.extract(r'(\\d+)')  # Extract numbers\n",
    "        \n",
    "        cabin_number = pd.to_numeric(number[0], errors='coerce')\n",
    "        \n",
    "        # Combine deck and scaled cabin number into a DataFrame\n",
    "        transformed = pd.DataFrame({\n",
    "            'CabinDeck': deck,\n",
    "            'CabinNumber': cabin_number\n",
    "        })\n",
    "\n",
    "        return transformed\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # After transform, this transformer outputs a single column \"Title\"\n",
    "        return np.array([\"CabinDeck\", \"CabinNumber\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26883b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_pipeline = Pipeline([\n",
    "    (\"imp\", SimpleImputer(strategy=\"most_frequent\")),  # Impute missing values with the most frequent value\n",
    "    (\"custom_transformer\", CabinTransformer())  # Apply the custom CabinTransformer\n",
    "])\n",
    "\n",
    "\n",
    "transformed_cabin = cabin_pipeline.fit_transform(df[[\"Cabin\"]])\n",
    "display(transformed_cabin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3411ae57",
   "metadata": {},
   "source": [
    "**ColumnTransformer in scikit-learn**\n",
    "\n",
    "`ColumnTransformer` is a utility in `scikit-learn` that allows you to apply different transformations to different subsets of columns in your dataset. It's particularly useful when working with datasets that contain various types of features (e.g., numerical, categorical, text) that require distinct preprocessing steps.\n",
    "\n",
    "In a machine learning pipeline, preprocessing is a crucial step to transform raw data into a suitable format for model training. The `ColumnTransformer` helps by enabling separate processing for different features, without needing to manually split the dataset or write custom logic.\n",
    "\n",
    "A `ColumnTransformer` allows you to specify a list of transformers to be applied to specific subsets of columns. The general format looks like this:\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "column_transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('name_of_transformer', transformer_object, columns_to_transform)\n",
    "    ]\n",
    ")\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **`name_of_transformer`**: This is a string label that names the transformation. It can be anything descriptive, like `'scaler'`, `'encoder'`, etc.\n",
    "\n",
    "- **`transformer_object`**: This is the transformer object that will be applied to the selected columns. This could be a `StandardScaler`, `SimpleImputer`, `OneHotEncoder`, etc.\n",
    "\n",
    "- **`columns_to_transform`**: A list or a string specifying which columns should be transformed. You can select columns by name, index, or condition (such as numerical columns).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b319902e",
   "metadata": {},
   "source": [
    "Before instantiating our custom transformations and pipelines, let's first outline the transformations we need to apply to each column in the dataset:\n",
    "\n",
    "- `PassengerId`: Drop column (since it's just an identifier and doesn't contribute to the model).\n",
    "\n",
    "- `Pclass`: Ordinal Encoding (using OrdinalEncoder) - This variable has a meaningful order (1st class > 2nd class > 3rd class), so it's encoded as integers while preserving the ordinal relationship.\n",
    "\n",
    "- `Sex`: Binary Encoding using OneHotEncoder (OneHotEncoder(drop=\"first\")) - This encodes male and female as binary values, with the first category dropped to avoid multicollinearity.\n",
    "\n",
    "- `Age`: Mean imputation (SimpleImputer(strategy=\"mean\")) followed by MinMax scaling (MinMaxScaler) - Missing values are filled with the mean age, and the values are then scaled to the range [0, 1].\n",
    "\n",
    "- `SibSp & Parch`: Custom transformation (FamilySizeTransformer) to create a new feature FamilySize, which is the sum of SibSp and Parch - This gives a better representation of family size, which may be more informative than the individual columns.\n",
    "\n",
    "- `Embarked`: Most frequent imputation (SimpleImputer(strategy=\"most_frequent\")) followed by OneHotEncoding (OneHotEncoder) - Missing values are imputed with the most frequent value, and then the categorical values are encoded using OneHotEncoding.\n",
    "\n",
    "- `Fare`: Applied StandardScaler - This variable is scaled to have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e151c46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NameTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        # Extract the Title from the Name column\n",
    "        names = X[\"Name\"]\n",
    "        titles = names.str.split(',', expand=True)[1].str.split('.', expand=True)[0]\n",
    "        return pd.DataFrame({\"Title\": titles})\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # After transform, this transformer outputs a single column \"Title\"\n",
    "        return np.array([\"Title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e319ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FamilySizeTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Compute family size\n",
    "        X_copy = X.copy()\n",
    "        X_copy['FamilySize'] = X_copy['SibSp'] + X_copy['Parch'] + 1\n",
    "        return X_copy[['FamilySize']]\n",
    "    \n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        # After transform, this transformer outputs a single column \"FamilySize\"\n",
    "        return np.array([\"FamilySize\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809b38b9",
   "metadata": {},
   "source": [
    "Now that we've defined our custom transformers, we can build individual pipelines for each feature or group of features. Each pipeline handles the specific preprocessing steps required for that column, such as imputation, scaling, encoding, or applying custom logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "pclass_pipeline = Pipeline([\n",
    "    (\"ohe\", OrdinalEncoder())\n",
    "])\n",
    "\n",
    "sex_pipeline = Pipeline([\n",
    "    (\"binary_ohe\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "])\n",
    "\n",
    "age_pipeline = Pipeline([\n",
    "    (\"mean_imp\", SimpleImputer(strategy=\"mean\")),\n",
    "    (\"mm_scaler\", MinMaxScaler())\n",
    "])\n",
    "\n",
    "family_pipeline = Pipeline([\n",
    "    (\"custom_trans\", FamilySizeTransformer())\n",
    "])\n",
    "\n",
    "embarked_pipeline = Pipeline([\n",
    "    (\"most_freq_imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\", sparse_output=False))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7561d1",
   "metadata": {},
   "source": [
    "After defining all individual pipelines, we can combine them into a single ColumnTransformer. This allows us to apply the appropriate preprocessing steps to each subset of features in a clean and organized way, all within one unified transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223d4561",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_transformation = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"pclass_preprocess\", pclass_pipeline, [\"Pclass\"]),\n",
    "        (\"sex_preprocess\", sex_pipeline, [\"Sex\"]),\n",
    "        (\"age_preprocess\", age_pipeline, [\"Age\"]),\n",
    "        (\"sibsp_parch_preprocess\", family_pipeline, [\"SibSp\", \"Parch\"]),\n",
    "        (\"embarked_preprocess\", embarked_pipeline, [\"Embarked\"]), \n",
    "        (\"fare_preprocess\", StandardScaler(), [\"Fare\"])\n",
    "        ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=True,\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a02b047",
   "metadata": {},
   "source": [
    "Before applying the transformation, we split the dataset into features (X) and the target variable (y). This ensures that the target variable (Survived) is not included in the transformations, as it should remain separate and only be used for model training or evaluation. The remainder=\"drop\" parameter in the ColumnTransformer ensures that any columns not explicitly specified for transformation are dropped from the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9392da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=[\"Survived\"]), df[\"Survived\"]\n",
    "\n",
    "transformed_array = final_transformation.fit_transform(X)\n",
    "feature_names = final_transformation.get_feature_names_out()\n",
    "\n",
    "transformed_df = pd.DataFrame(transformed_array, columns=feature_names, index=X.index)\n",
    "transformed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e834fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look duplicated rows \n",
    "print(\"Duplicated rows in the transformed DataFrame:\", transformed_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e482ee",
   "metadata": {},
   "source": [
    "**Why so many “duplicates”?**  \n",
    "After feature engineering and encoding, multiple distinct passengers end up with exactly the same values across all transformed features *and* the same survival label. For example, two female passengers in 3rd class, both aged 24 (imputed or binned the same), with identical family size and embarkation port - all of those collapse to an identical feature vector. These are *not* data errors but simply reflect that our pipeline maps different raw records to the same engineered profile.\n",
    "\n",
    "Having validated that our final dataset is clean, complete, and fully numeric, we’re now ready to move on to model building and evaluation in Lab 2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5acce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach the target column to the transformed DataFrame\n",
    "transformed_df[\"Survived\"] = y.values\n",
    "\n",
    "# save the final DataFrame to a CSV file\n",
    "transformed_df.to_csv(\"../Lab-02/data/my_titanic.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ec2c61",
   "metadata": {},
   "source": [
    "## EXTRA EXERCISE -- Unsupervised Exploration: PCA & t-SNE\n",
    "\n",
    "When we have many numeric features, it can be hard to visualize or understand the “shape” of the data in its full dimensionality. Two popular techniques—**PCA** and **t-SNE**—help us project high-dimensional data into 2D (or 3D) so we can:\n",
    "\n",
    "- **See clusters** or separations between classes.  \n",
    "- **Detect outliers** or unexpected structure.  \n",
    "- **Decide** if reduced representations might be useful for modeling or further feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efb43",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is a **linear** technique that finds new orthogonal axes (principal components) that capture the most variance in the data.\n",
    "\n",
    "1. **Standardize** each feature to mean 0, variance 1.  \n",
    "2. **Compute** the covariance matrix of the standardized data.  \n",
    "3. **Eigen-decompose** → get eigenvectors (directions) and eigenvalues (variance explained).  \n",
    "4. **Order** components by descending eigenvalue and **project** your data onto the top _k_ components.\n",
    "\n",
    "**Why it helps in EDA/preprocessing**  \n",
    "- You can plot the first two PCs and color by `Survived` to see if survivors cluster apart.  \n",
    "- The **explained-variance ratio** tells you how many components capture, say, 90% of the variance—guiding dimensionality reduction.  \n",
    "- By comparing model performance on the top _n_ PCs vs. all features, you learn how much information lives in those directions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddd96b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Select and standardize numeric columns\n",
    "num_cols = transformed_df.drop(columns=\"Survived\").select_dtypes(include=\"number\").columns\n",
    "X_num    = transformed_df[num_cols].values\n",
    "X_scaled = StandardScaler().fit_transform(X_num)\n",
    "\n",
    "# 2. Fit PCA\n",
    "pca    = PCA(n_components=2)\n",
    "X_pca2 = pca.fit_transform(X_scaled)\n",
    "\n",
    "# 3. Explained variance\n",
    "print(\"Explained variance ratio (2 PCs):\", pca.explained_variance_ratio_)\n",
    "\n",
    "# 4. Plot explained-variance ratio\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([1, 2], pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance (2 PCs)')\n",
    "plt.xticks([1,2], ['PC1','PC2'])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Scatter plot of the first two PCs, colored by survival\n",
    "survived = transformed_df['Survived'].values\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(\n",
    "    X_pca2[survived == 0, 0],\n",
    "    X_pca2[survived == 0, 1],\n",
    "    label='Did Not Survive'\n",
    ")\n",
    "plt.scatter(\n",
    "    X_pca2[survived == 1, 0],\n",
    "    X_pca2[survived == 1, 1],\n",
    "    label='Survived'\n",
    ")\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA Projection by Survival')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c43001a",
   "metadata": {},
   "source": [
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "t-SNE is a **non-linear** technique that excels at preserving **local** structure (neighborhoods) in a 2D embedding.\n",
    "\n",
    "1. Compute pairwise similarities in high-dim space using Gaussian probabilities.\n",
    "\n",
    "2. Define pairwise similarities in low-dim space with a heavy-tailed Student’s t distribution.\n",
    "\n",
    "3. Optimize the embedding by minimizing the Kullback–Leibler divergence between the two distributions via gradient descent.\n",
    "\n",
    "**Why it helps in EDA/preprocessing**\n",
    "\n",
    "- It often reveals clusters or subpopulations that PCA (linear) might miss.\n",
    "\n",
    "- Good for visualization, but not typically used as a direct input to models (it doesn’t preserve global distances).\n",
    "\n",
    "- Requires tuning perplexity (roughly, neighborhood size) and learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc302f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Fit t-SNE on your standardized numeric data\n",
    "tsne    = TSNE(n_components=2, random_state=42)\n",
    "X_tsne2 = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# 2. Report the shape\n",
    "print(\"t-SNE embedding shape:\", X_tsne2.shape)\n",
    "\n",
    "# 3. Scatter plot of the embedding, colored by survival\n",
    "survived = transformed_df['Survived'].values\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "plt.scatter(\n",
    "    X_tsne2[survived == 0, 0],\n",
    "    X_tsne2[survived == 0, 1],\n",
    "    label='Did Not Survive'\n",
    ")\n",
    "plt.scatter(\n",
    "    X_tsne2[survived == 1, 0],\n",
    "    X_tsne2[survived == 1, 1],\n",
    "    label='Survived'\n",
    ")\n",
    "plt.xlabel('t-SNE Component 1')\n",
    "plt.ylabel('t-SNE Component 2')\n",
    "plt.title('t-SNE Projection of Numeric Features by Survival')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
